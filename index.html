<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <title>AAAI 2021 Workshop: Towards Robust, Secure and Efficient Machine Learning (RSEML) </title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link href="css/style.css" rel="stylesheet" type="text/css"/>

    <style>
        
        .logo {
            display: inline-block;
            text-align: center;
            margin: 20px;
            height: auto;
        }

        .logophoto img {
            height: 50px;
            align-content: center;
            border-radius: 0px;
            margin-bottom: 10px;
        }

        .subguide {
            text-align: left;
        }

        body {
            /*background: #f7f7f7;*/
            background: #e3e5e8;
            color: #f7f7f7;
            font-family: 'Lato', Verdana, Helvetica, sans-serif;;
            font-weight: 300;
            font-size:16px;
        }

    </style>
</head>

<body>

<div class="container">
    <p><img src="https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2017/09/banner_virtual.png" width="1000" align="middle" /></p>
    <table border="0" align="center">
        <tr>
            <td width="1000" align="center" valign="middle"><h2>AAAI 2021 Workshop</h2>
                <span class="title"><strong>Towards Robust, Secure and Efficient Machine Learning</strong></span></td>
        </tr>
        <tr>
        <td colspan="3" align="center"><h3>Venue: <a href="https://zoom.us/j/93678598207?pwd=NnFDTWlHUzIvSWdWalZUZG45bzB5Zz09">Online</a><br><br>February 8, 2021 (16:00 - 21:00 PST)<br><br>
        </tr>
    </table>
</div>

</br>

<div class="container">
    <h2>Overview</h2>
    <div class="overview">
        <p>Machine learning technology has been improving with every passing day and has been extensively applied to nearly every
corner of the society that offers substantial benefits to our daily lives. However, machine learning models face various
threats. For example, it is known that machine learning models are vulnerable to adversarial samples. The existence of
adversarial examples reveals that current machine learning models are vulnerable and can be easily fooled, leading to serious
security concerns in machine learning systems such as autonomous driving vehicles or face recognition systems.</p>

        <p>More recently, due to both data privacy requirements as specified in the European Union’s General Data Protection
Regulation (GDPR), and the limitations of computation power, the training process of machine learning models has extended
from centralized to decentralized (i.e. distributed or federated learning) where the model will suffer from even more threats.
For example, in a federated learning setting, every client can perform various attacks such as backdoor attacks on the global
model as clients have direct access to the global model. How to prevent privacy leaking during information exchange of a
decentralized training method is also a critical issue.</p>

        <p>At the same time, computation efficiency is a big concern for modern deep learning, both inference and training. For
inference, people prefer inference on edge devices due to better privacy, but edge devices have very limited computational
resource. For training, gradient or weight exchange is necessary for decentralized training, but such exchange requires
communication, which may be slow. Furthermore, models that are robust to adversarial attacks usually require longer
training time and orders of magnitude more computation FLOPs than normal networks.</p>

    <p>This one-day workshop intends to bring experts from machine learning, security communities, and federated learning
together to work more closely in addressing the posed concerns. Specifically, we seek to study threats and defenses to
machine learning not only in a single node setting but also in a distributed setting. In summary, we seek solutions to 
achieve a wholistic solution for robust, secure and efficient machine learning.</p>

    </div>
</div>

</br>

<div class="container">
    <h2 id="accepted">Accepted Papers</h2>
    <div class="topicinterests">
      <ol>
        <li>Alberto Matachana, Kenneth Co, Luis Muñoz-González, David Martinez and Emil Lupu. <a href="https://arxiv.org/abs/2012.06024">Robustness and Transferability of Universal Attacks on Compressed Models</a></li>
        <li>Arnaud Van Looveren and Giovanni Vacanti. <a href="https://arxiv.org/abs/2002.09364">Adversarial Detection and Correction by Matching Prediction Distributions</a></li>
        <li>Arno Blaas and Stephen Roberts. <a href="https://arxiv.org/abs/2101.02689">The Effect of Prior Lipschitz Continuity on the Adversarial Robustness of Bayesian Neural Networks</a></li>
        <li>Chang Song, Elias Fallon and Hai Li. <a href="https://arxiv.org/abs/2012.14965">Improving Adversarial Robustness in Weight-quantized Neural Networks</a></li>
        <li>Dashan Gao, Ben Tan, Ce Ju, Vincent W. Zheng and Qiang Yang. <a href="#">Federated Factorization Machine for Secure Recommendation with Sparse Data</a></li>
        <li>Hang Chen, Syed Ali Asif, Jihong Park, Chien-Chung Shen and Mehdi Bennis. <a href="https://arxiv.org/abs/2101.03300">Robust Blockchained Federated Learning with Model Validation and Proof-of-Stake Inspired Consensus</a></li>
        <li>Jin-woo Lee, Jaehoon Oh, Sungsu Lim, Se-Young Yun and Jae-Gil Lee. <a href="https://arxiv.org/abs/2012.03214">TornadoAggregate: Accurate and Scalable Federated Learning via the Ring-Based Architecture</a></li>
        <li>Marissa Dotter, Keith Manville, Josh Harguess, Colin Busho and Mikel Rodriguez. <a href="http://arxiv.org/abs/2101.02899">Adversarial Attack Attribution: Discovering Attributable Signals in Adversarial ML Attacks</a></li>
        <li>Mengting Xu, Tao Zhang, Zhongnian Li, Wei Shao and Daoqiang Zhang. <a href="https://arxiv.org/abs/2012.13103">Improving the Certified Robustness of Neural Networks via Consistency Regularization</a></li>
        <li>Mohammadreza Ebrahimi, Ning Zhang, James Hu, Muhammad Taqi Raza and Hsinchun Chen. <a href="https://arxiv.org/abs/2012.07994">Binary Black-box Evasion Attacks Against Deep Learning-based Static Malware Detectors with Adversarial Byte-Level Language Model</a></li>
        <li>Nasser Aldaghri, Hessam Mahdavifar and Ahmad Beirami. <a href="https://arxiv.org/abs/2012.15721">Coded Machine Unlearning</a></li>
        <li>Ruixuan Luo, Wei Li, Zhiyuan Zhang, Ruihan Bao, Keiko Harimoto and Xu Sun. <a href="https://arxiv.org/abs/2012.13489">Learning Robust Representation for Clustering through Locality Preserving Variational Discriminative Network</a></li>
        <li>Sheng Jia, Ehsan Nezhadarya, Yuhuai Wu and Jimmy Ba. <a href="#">Efficient Outlier Detection and Statistical Tests: A Neural Tangent Kernel Approach</a></li>
        <li>Shuhao Fu, Chulin Xie, Bo Li and Qifeng Chen. <a href="https://arxiv.org/abs/1912.11464">Attack-Resistant Federated Learning with Residual-based Reweighting</a></li>
        <li>Sungkwon An, Jeonghoon Kim, Myungjoo Kang, Shahbaz Razaei and Xin Liu. <a href="https://arxiv.org/abs/2101.02358">OAAE: Adversarial Autoencoders for Novelty Detection in Multi-modal Normality Case via Orthogonalized Latent Space</a></li>
        <li>Tomohiro Hayase, Suguru Yasutomi and Takashi Kato. <a href="https://arxiv.org/abs/2012.11849">Selective Forgetting of Deep Networks at a Finer Level than Samples</a></li>
        <li>Utkarsh Uppal and Bharat Giddwani. <a href="https://arxiv.org/abs/2012.06876">Normalized Label Distribution: Towards Learning Calibrated, Adaptable and Efficient Activation Maps</a></li>
        <li>Xiaoyang Wang, Bo Li, Jacky Zhang, Bhavya Kailkhura and Klara Nahrstedt. <a href="https://arxiv.org/abs/2101.05950">Robusta: Robust AutoML for Feature Selection via Reinforcement Learning</a></li>
        <li>Yi Zhu, Yiwei Zhou and Menglin Xia. <a href="https://arxiv.org/abs/2005.12696">Generating Semantically Valid Adversarial Questions for TableQA</a></li>
        <li>Yuting Liang and Reza Samavi. <a href="https://arxiv.org/abs/2007.01507">Towards Robust Deep Learning With Ensemble Networks and Noisy Layers</a></li>        
      </ol>
    </div>
</div>

</br>

<!-- <div class="container">
    <h2>Important Dates</h2>
    <div class="importantdate">
      <p>Submission Due &nbsp &nbsp <strong>Sunday, November 15, 2020 (extended from November 9)</strong></p>
      <p>Notifications Due &nbsp &nbsp<strong>Monday, December 7, 2020 (extended from November 30)</strong></p>
    </div>
</div> -->

<div class="container">
    <h2>Program Schedule</h2>
    <div class="importantdate">
        <table style="width:100%; border-spacing: 5px">
            <colgroup>
                <col span="1" style="width: 12%;">
                <col span="1" style="width: 88%;">
             </colgroup>

            <tbody>
            <tr>
                <th style="text-align: center;"><b>Time (PST)</b></th>
                <th><b>Activity</b></th>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">15:45 – 16:00</td>
                <td style="vertical-align:top">Presenters to connect and test the system</td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">16:00 – 16:05</td>
                <td style="vertical-align:top"><b>Opening Remark</b></td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">16:05 – 16:35</td>
                <td style="vertical-align:top">Keynote Session 1: <a href="#talkkurt">Efficiency is the Key to Privacy (and Security) by Prof. Kurt</a></td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">16:35 – 17:15</td>
                <td style="vertical-align:top"><b>Technical Talks Session 1</b> (2 talks, 20 mins each including Q&amp;A)
                <ol>
                    <li>Alberto Matachana, Kenneth Co, Luis Muñoz-González, David Martinez and Emil Lupu. Robustness and Transferability of Universal Attacks on Compressed Models</li>
                    <li>Yi Zhu, Yiwei Zhou and Menglin Xia. Generating Semantically Valid Adversarial Questions for TableQA</li>
                </ol>
                </td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">17:15 – 17:20</td>
                <td style="vertical-align:top"><b>Break</b> (Presenters should connect and test the system)</td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">17:20 – 17:50</td>
                <td style="vertical-align:top">Keynote Session 2: <a href="#talkhuang">Vertical Federated Kernel Learning by Prof. Huang</td>
            </tr>
            <tr>
            <td align="center" style="vertical-align:top">17:50 – 18:30</td>
                <td style="vertical-align:top"><b>Technical Talks Session 2</b> (2 talks, 20 mins each including Q&amp;A)
                <ol>
                    <li>Shuhao Fu, Chulin Xie, Bo Li and Qifeng Chen. Attack-Resistant Federated Learning with Residual-based Reweighting</li>
                    <li>Chang Song, Elias Fallon and Hai Li. Improving Adversarial Robustness in Weight-quantized Neural Networks</li>
                </ol>
            </td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">18:30 – 18:35</td>
                <td style="vertical-align:top"><b>Break</b> (Presenters should connect and test the system)</td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">18:35 – 19:05</td>
                <td style="vertical-align:top">Keynote Session 3: <a href="#talklaurens">TBC by Dr. Laurens</a></td>
            </tr>
            <tr>
            <td align="center" style="vertical-align:top">19:05 – 19:45</td>
                <td style="vertical-align:top"><b>Technical Talks Session 3</b> (2 talks, 20 mins each including Q&amp;A)
                <ol>
                    <li>Xiaoyang Wang, Bo Li, Jacky Zhang, Bhavya Kailkhura and Klara Nahrstedt. Robusta: Robust AutoML for Feature Selection via Reinforcement Learning</li>
                    <li>Nasser Aldaghri, Hessam Mahdavifar and Ahmad Beirami. Coded Machine Unlearning</li>
                </ol>
                </td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">19:45 – 20:55</td>
                <td style="vertical-align:top">Poster Session
                <ul>
                    <li><a href="#accepted">All accepted papers</a></li>
                </ul>
                </td>
            </tr>
            <tr>
                <td align="center" style="vertical-align:top">20:55 - 21:00</td>
                <td style="vertical-align:top"><b>Closing Ceremony</b></td>
            </tr>
          </tbody>
        </table>
    </div>
</div>

</br>

<div class="container">
    <h2>Invited Speaker</h2>
    <div class="oc">
        <ul>
            <li id="talkkurt">
                <a href="https://people.eecs.berkeley.edu/~keutzer/">Kurt Keutzer, University of California, Berkeley</a>
                
                <br/>
                <br/>
                <u>Title:</u> <strong>Efficiency is the Key to Privacy (and Security)</strong>
                <br/>
                <u>Time:</u> <strong>16:05-16:35 PST</strong>
                
                <br/>
                <br/>
                <u>Abstract:</u> Given the numerous reported (and unreported) security breaches, as well as pervasive concerns around use of private data, 
                local processing of personal data associated with speech, cameras, and language appears to be the only completely effective way to ensure security and privacy. 
                However, performing these computations without access to the cloud requires significant improvements in the efficiency of Deep Learning at the edge.
                In this talk we will discuss our efforts to move one of the most computationally challenging problems, Natural Language Understanding, to the edge. 
                We will also share some initial thoughts on addressing an even more challenging problem, recommendation systems, to mobile clients.                
                
                <br/>
                <br/>
                <u>Biography:</u> Kurt’s research at University of California, Berkeley, focuses on computational problems in Deep Learning. 
                In particular, Kurt has worked to reduce the training time of ImageNet and BERT to minutes, and with the “Squeeze” family of Deep Neural nets, 
                he has endeavored to develop a family of DNNs suitable for mobile and IOT applications. Before joining Berkeley as a Full Professor in 1998, 
                Kurt was CTO and SVP at Synopsys. Kurt’s earlier contributions to Electronic Design Automation were recognized at the 50th Design Automation 
                Conference where he was noted as a Top 10 most cited author, as an author of a Top 10 most cited paper, and as one of only three people to 
                have won four Best Paper Awards at that conference.  Kurt was named a Fellow of the IEEE in 1996. 
                
                <br/>
                <br/>
            </li>
            <li id="talkhuang">
                <a href="http://www.pitt.edu/~heh45/">Heng Huang, JD.com and University of Pittsburgh</a>

                <br/>
                <br/>
                <u>Title:</u> <strong>Vertical Federated Kernel Learning</strong>
                <br/>
                <u>Time:</u> <strong>17:20-17:50 PST</strong>
                
                <br/>
                <br/>
                <u>Abstract:</u> Vertically partitioned data widely exist in the modern data mining and machine learning applications, 
                where data are provided by multiple providers (companies, stakeholders, government departments, etc.) and each maintains 
                the records of different feature sets with common entities. For example, a digital finance company, an E-commerce company, 
                and a bank collect different information about the same person. The digital finance company has access to the online consumption, 
                loan and repayment information. The E-commerce company has access to the online shopping information. The bank has customer information 
                such as average monthly deposit, account balance. If the person submits a loan application to the digital finance company, 
                the digital finance company might evaluate the credit risk to this financial loan by comprehensively utilizing the information 
                stored in the three parts. However, direct access to the data in other providers or sharing of the data is often prohibited due 
                to legal and commercial reasons. It is challenging to train these vertically partitioned data effectively and efficiently while 
                keeping data privacy for traditional machine learning algorithms.                 
                <br/>
                <br/>
                Vertical federated learning has been employed as good solution to address such situations. However, most existing vertical federated 
                learning methods are linear models. To improve the prediction performance, we focus on nonlinear learning with kernels, and propose 
                a federated doubly stochastic kernel learning (FDSKL) algorithm for vertically partitioned data. Specifically, we use random features 
                to approximate the kernel mapping function and use doubly stochastic gradients to update the solutions, which are all computed federatedly 
                without the disclosure of data. Importantly, we prove that FDSKL has a sublinear convergence rate, and can guarantee the data security 
                under the semi-honest assumption. Extensive experimental results on a variety of benchmark datasets show that FDSKL is significantly 
                faster than state-of-the-art federated learning methods when dealing with kernels, while retaining the similar generalization performance.

                <br/>
                <br/>
                <u>Biography:</u> Dr. Heng Huang is John A. Jurenko Endowed Professor in Electrical and Computer Engineering at University of Pittsburgh, 
                and is also a consulting researcher with JD Finance American Corporation. Dr. Huang received the PhD degree in Computer Science at Dartmouth 
                College. His research areas include machine learning, big data mining, and biomedical data science. Dr. Huang has published more than 230 
                papers in top-tier conferences and many papers in premium journals, such as ICML, NeurIPS, KDD, RECOMB, ISMB, ICCV, CVPR, IJCAI, AAAI, TPAMI, JMLR, etc. 
                Based on csrankings.org, for the last ten years, Dr. Huang is ranked 2nd among researchers in U.S. who published most top computer science 
                conference papers. He is a Fellow of AIBME and serves as the Program Chair of ACM SIGKDD Conference 2020.

                <br/>
                <br/>
            </li>
            <li id="talklaurens">
                <a href="https://lvdmaaten.github.io/">Laurens van der Maaten, Facebook AI Research (FAIR)</a>

                <br/>
                <br/>
                <u>Title:</u> <strong>TBC</strong>
                <br/>
                <u>Time:</u> <strong>18:35-19:05 PST</strong>
                
                <!-- <br/>
                <br/>
                <u>Abstract:</u>

                <br/>
                <br/>
                <u>Biography:</u> 

                <br/>
                <br/> -->
            </li>
        </ul>
    </div>
</div>

</br>
<div class="container">
    <h2>Topic of Interests</h2>
    <div class="topicinterests">
      Topics including (but not limit to): 
      <!-- <ul>
        <li>Adversarial learning, data poisoning, adversarial examples, adversarial robustness, and black box attacks. </li>
        <li>Improving model robustness against various attacks such as evasion, data poisoning, model inversion or backdoor attacks. </li>
        <li>Theoretical contributions of adversarial machine learning. </li>
        <li>Interpretable machine learning through adversarial learning. </li>
        <li>Differential privacy in distributed machine learning. </li>
        <li>Privacy concerns in distributed machine learning. </li>
        <li>Adversarial distributed machine learning. </li>
        <li>Adversarial transferability in distributed machine learning. </li>
        <li>Federated learning and distributed privacy preserving algorithms. </li>
        <li>Improving model performance of general tasks via adversarial machine learning (e.g., image classification, speech recognition) </li>
        <li>Efficiency improvement in distributed machine learning. </li>
        <li>Trade-off between privacy and efficiency. </li>
      </ul> -->
      <ul>
            <li>Theoretical contributions of adversarial machine
            learning.</li>
            <li>Training data poisoning and adversarial learning.</li>
            <li>Adversarial attacks (e.g. evasion) and defenses.</li>
            <li>Secure machine learning.</li>
            <li>Privacy-preserving machine learning.</li>
            <li>Applications of privacy-preserving machine learning.</li>
            <li>Privacy attacks such as membership inference, and model inversion.</li>
            <li>Secure multi-party computation.</li>
            <li>Model compression and efficiency improvement in both training and inference.</li>
            <li>Efficiency improvement of information exchange in distributed training.</li>
            <li>Model robustness against model compression.</li>
      </ul>
    </div>
</div>

</br>

<div class="container">
    <h2>Submission Guidelines</h2>
    <div class="subguide">
      <p>Submissions can be a <strong>full technical paper (up to 8 pages) or short paper (up to 4 pages)</strong> excluding references or 
        supplementary materials.</p> 
      <p>Authors should only rely on the supplementary material to include minor details that do not fit in the main paper.</p>
      <p>The submissions are anonymous for double-blind review.</p>
      <!-- <p>The accepted papers will be posted on the workshop website and will not appear in the AAAI proceedings. </p> -->
      <p>The workshop will not have formal proceedings.</p> 
      <!-- , but authors of accepted papers can choose to have their work published on <strong>the workshop website or arXiv by themselves or none.</strong></p> -->
      <p>Please follow AAAI 2021 Latex style for paper format.</p>
      <p>The final submission must be in PDF and please submit your papers to <a href="https://easychair.org/conferences/?conf=rseml2021">https://easychair.org/conferences/?conf=rseml2021</a></p>
    </div>
</div>
  
</br>


<div class="container">
    <h2>Organizing Committee</h2>
    <div class="oc">
        <p><strong>General Chair</strong></p>
        <ul>
            <li>Qiang Yang, WeBank and Hong Kong University of Science and Technology</li>
        </ul>

        <p><strong>Program Chair</strong></p>
        <ul>
            <li><a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song, University of California, Berkeley</a></li>
            <li><a href="https://songhan.mit.edu/">Song Han, Massachusetts Institute of Technology</a></li>
            <li><a href="https://scholar.google.fi/citations?user=fOsgdn0AAAAJ&hl=en">Lixin Fan, WeBank</a></li>
            <li><a href="https://www.ntu.edu.sg/home/han.yu/">Han Yu, Nanyang Technological University</a></li>
        </ul>

        <p><strong>Program Committee</strong></p>
        <ul>
            <li><a href="http://people.cs.uchicago.edu/~ravenben">Ben Y. Zhao, University of Chicago</a></li>
            <li><a href="https://ece.duke.edu/faculty/yiran-chen">Yiran Chen, Duke University</a></li>
            <li><a href="http://www-sop.inria.fr/members/Giovanni.Neglia/">Giovanni Neglia, Inria</a></li>
            <li><a href="https://sites.google.com/site/researchjianguoyao/">Jianguo Yao, Shanghai Jiao Tong</a></li>
            <li><a href="https://mila.quebec/en/person/jian-tang/">Jian Tang, MILA</a></li>
            <li><a href="http://jakubkonecny.com/">Jakub Konečný, Google</a></li>
            <li><a href="https://ruoxijia.info/research/">Ruoxi Jia, Virginia Tech</a></li>
            <li><a href="https://www.cse.ust.hk/admin/people/faculty/profile/kaichen">Kai Chen, Hong Kong University of Science and Technology</a></li>
            <li><a href="https://www.cs.jhu.edu/~vova/">Vladimir Braverman, Johns Hopkins University</a></li>
            <li><a href="https://www.avestimehr.com/">Salman Avestimehr, University of Southern California</a></li>
            <li><a href="https://sites.google.com/site/yangliuveronica/">Yang Liu, WeBank</a></li>
            <li><a href="http://cs-chan.com/">Chee Seng Chan, University of Malaya</a></li>
            <li><a href="https://sites.google.com/view/btan/home">Ben Tan, WeBank</a></li>
            <li><a href="https://cold-winter.github.io/">Yandong Li, University of Central Florida</a></li>
            <li><a href="https://yangzhang4065.github.io/">Yang Zhang, Alibaba Seattle</a></li>
            <li><a href="https://chaoyanghe.com/">Chaoyang He, University of Southern California</a></li>
            <li><a href="https://richtarik.org/">Peter Richtárik, KAUST</a></li>
            <li><a href="https://jungyhuk.github.io/">Xinyun Chen, UC, Berkeley</a></li>
            <li><a href="http://scholar.google.com/citations?user=HoAPJHoAAAAJ&hl=en">Jia Huei Tan, University of Malaya</a></li>
            <li><a href="https://pralab.diee.unica.it/en/MauraPintor">Maura Pintor, University of Cagliari</a></li>
            <li><a href="https://sites.google.com/view/jalajupadhyay/home">Jalaj Upadhyay, Johns Hopkins University</a></li>
            <li><a href="http://www.mit.edu/~yuzhe/">Yuzhe Yang, MIT</a></li>
            <li><a href="https://kikacaty.github.io/">Yulong Cao, University of Michigan</a></li>
            <li><a href="http://www.princeton.edu/~abhagoji/index.html">Arjun Nitin Bhagoji, Princeton University</a></li>
            <li><a href="http://research.ntu.edu.sg/expertise/academicprofile/pages/StaffProfile.aspx?ST_EMAILID=JUNZHAO">Jun Zhao, Nanyang Technological University</a></li>
            <li><a href="https://pralab.diee.unica.it/en/AmbraDemontis">Ambra Demontis, University of Cagliari</a></li>
            <li><a href="https://hongyanz.github.io/">Hongyang Zhang, TTIC</a></li>
            <li><a href="https://www.cs.umd.edu/~chenzhu/">Chen Zhu, University of Maryland</a></li>
            <li><a href="https://www.danxurgb.net/">Dan Xu, HKUST</a></li>
            <li><a href="https://yingwei.li/">Yingwei Li, Johns Hopkins University</a></li>
            <li><a href="https://www.uts.edu.au/staff/guodong.long">Guodong Long, University of Technology, Sydney</a></li>
            <li><a href="http://zhijianliu.com/">Zhijian Liu, MIT</a></li>
            <li><a href="https://dashangao.github.io/">Dashan Gao, HKUST</a></li>
            <li><a href="#">Sheng Wan, HKUST</a></li>
        </ul>
        
        <p><strong>Industrial Chair</strong></p>
        <ul>
            <li>Ce Ju, WeBank</li>
            <li>Ruihui Zhao, Tencent</li>
        </ul>

        <p><strong>Publicity Chair</strong></p>
        <ul>
            <li>Tianyu Zhang, WeBank</li>
            <li><a href="https://kl4805.github.io/">Yilun Jin, Hong Kong University of Science and Technology</a></li>
            <li>Kam Woh Ng, WeBank (please contact <a href="mailto:aaai2021ws.rseml@gmail.com">us</a> or <a href="mailto:kamwoh@gmail.com">Kam Woh</a> if you have any questions)</li>
        </ul>
        
        <!-- <div  class="smallword">
            <p>* sorted by first name.</p>
        </div> -->
    </div>
</div>

</br>

<div class="container">
    <h2>Sponsored by</h2>
    <div>
        <div class="instructor" style="width: 240px">
            <a href="http://aij.ijcai.org/">
                <div class="logophoto"><img style="height: 70px;" src="http://aij.ijcai.org/files/images/ARTINT_Logo2_c_web_more.jpg"></div>
                <div>Artificial Intelligence Journal (AIJ)</div>
            </a>
        </div>
    </div>
</div>
</br>

<div class="container">
    <h2>Organized by</h2>
    <div>
        <div class="logo">
            <a href="https://www.webank.com/#/home">
                <div class="logophoto"><img src="http://fl-ijcai20.federated-learning.org/FL-IJCAI20/WeBank.jpg"></div>
                <!-- <div>WeBank</div> -->
            </a>
        </div>

        <div class="logo">
            <a href="https://www.ntu.edu.sg/Pages/home.aspx">
                <div class="logophoto"><img src="http://fl-ijcai20.federated-learning.org/FL-IJCAI20/NTU.png"></div>
                <!-- <div>Nanyang Technological University</div> -->
            </a>
        </div>

        <div class="logo">
            <a href="https://www.berkeley.edu/">
                <div class="logophoto"><img src="https://brand.berkeley.edu/wp-content/uploads/2016/11/primarylogo.png"></div>
                <!-- <div>University of California, Berkeley</div> -->
            </a>
        </div>

        <div class="logo">
            <a href="http://www.mit.edu/">
                <div class="logophoto"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAATgAAAChCAMAAABkv1NnAAAAVFBMVEX///+jHzSKi4zJi5KgDCieACLct7v++/vy4uSdABmiGC+xTluiDiugoKGCg4Tr6+vv3N+7vLyzV2Lq09XGhY3btLmbABGgACWcAA3Hx8jz8/N9fn8CugTJAAABuUlEQVR4nO3cSU7DQABFQUMCwcxDwnj/e3KBVlp6KxLqrf2tdm0t9XIx6363DNpsJ7Or69HsZp3Mbu9Gs+X1drJbn4e766vp98WW6RPghoGLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYudCNzdx/3xPsdwh3Wyy50I3LKbNZ7Nd7VTgft7gYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi4GLgYuBi50G3NfLrO/h7m2/Od5+eMzzgXt4vDzez9Nwd3jfHm/dnzncxO3ycQw3/T243YADBw4cOHDgwIEDBw4cOHDgwIEDBw4cOHDgwIEDBw4cOHDgwIEDBw4cOHDgwIEDBw4cOHDgwIEDBw4cOHDgwIEDBw4cOHDgwIH7h3D7dXKhx/sB3PBE0ytk3sDVVw8CFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXAxcDFwMXOx+4X+3ax1CGyRluAAAAAElFTkSuQmCC"></div>
                <!-- <div>Massachusetts Institute of Technology</div> -->
            </a>
        </div>
    </div>
</div>
</br>

<div class="containersmall">
    <p>Please do not hesitate to contact <a href="mailto:aaai2021ws.rseml@gmail.com">us</a> or <a href="mailto:kamwoh@gmail.com">Kam Woh</a> if you have questions. 
        This website is linked with <a href="http://federated-learning.org/rseml2021/">http://federated-learning.org/rseml2021/</a>.</p>
    
    <p>The webpage template is by the courtesy of 
        <a href="https://interpretablevision.github.io/">ICCV 2019 Tutorial on Interpretable
            Machine Learning for Computer Vision</a>.</p>
</div>

<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>